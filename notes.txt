python -m venv env
source env/bin/activate

tear down the virtual environment: deactivate

pip install -r requirements.txt

git clone --depth 1 --filter=blob:none --sparse https://github.com/mlflow/mlflow.git
cd mlflow
git sparse-checkout set docker-compose
cd docker-compose
docker compose up -d

docker-compose config

docker-compose logs -f mlflow

stop the docker setup: docker-compose down -v # # The -v flag removes volumes storing data

cd ../../

export MLFLOW_TRACKING_URI="http://localhost:5000"
echo $MLFLOW_TRACKING_URI
export OLLAMA_BASE_URI="http://localhost:11434"
echo $OLLAMA_BASE_URI

mkdir evaluations
cd evaluations/
touch evaluate_ollama.py

insert the code:

-----code-----
import mlflow
import pandas as pd
from langchain_ollama import OllamaLLM

# Set experiment
mlflow.set_experiment("Ollama Model Evaluation - Simple")

# Prepare data
eval_data = pd.DataFrame({
    "question": [
        "What is MLflow?",
        "What is the capital of Spain?",
        "Explain machine learning in simple terms.",
        "What is 2+2+2?",
        "Who wrote Romeo and Juliet?",
        "How many vowels are in Alabama?",
        "Which city is meant in the song \"We built this city\" by the group Starship?",
    ],
    "ground_truth": [
        "MLflow is an open-source platform for managing the machine learning lifecycle.",
        "The capital city of Spain is Madrid",
        "Machine learning is a way for computers to learn patterns from data.",
        "6",
        "William Shakespeare",
        "4",
        "Two cities are referenced: San Francisco and Los Angeles.",
    ]
})

# List of models to evaluate
models_to_evaluate = [
    "llama3.2:latest",
    "mistral:latest",
    "dolphin3:latest",
    "deepscaler:latest",
    "deepseek-r1:latest"
]

# Evaluate each model
for model_name in models_to_evaluate:
    print(f"\n{'='*60}")
    print(f"Evaluating model: {model_name}")
    print(f"{'='*60}\n")
    
    # Initialize model and generate predictions
    llm = OllamaLLM(model=model_name, base_url="http://localhost:11434")
    
    print("Generating predictions...")
    predictions = []
    for idx, question in enumerate(eval_data["question"], 1):
        print(f"[{idx}/{len(eval_data)}] Processing: {question[:60]}...")
        response = llm.invoke(question)
        predictions.append(response)
    
    eval_data["prediction"] = predictions
    
    # Run evaluation
    with mlflow.start_run(run_name=f"{model_name}-simple-eval"):
        mlflow.log_param("model", model_name)
        mlflow.log_param("provider", "ollama")
        mlflow.log_param("num_samples", len(eval_data))
        
        # Log the evaluation data as a table
        mlflow.log_table(data=eval_data, artifact_file="eval_results.json")
        
        # Calculate simple metrics manually
        from difflib import SequenceMatcher
        
        similarities = []
        for pred, truth in zip(eval_data["prediction"], eval_data["ground_truth"]):
            similarity = SequenceMatcher(None, pred.lower(), truth.lower()).ratio()
            similarities.append(similarity)
        
        avg_similarity = sum(similarities) / len(similarities)
        
        mlflow.log_metric("avg_similarity", avg_similarity)
        mlflow.log_metric("num_predictions", len(predictions))
        
        print(f"\n{'='*60}")
        print(f"Evaluation Complete for {model_name}!")
        print(f"{'='*60}")
        print(f"Average Similarity: {avg_similarity:.4f}")
        print(f"Number of samples: {len(predictions)}")
        
        print(f"\n{'='*60}")
        print("Sample Results:")
        print(f"{'='*60}")
        for i in range(min(3, len(eval_data))):
            print(f"\nQuestion: {eval_data.iloc[i]['question']}")
            print(f"Ground Truth: {eval_data.iloc[i]['ground_truth']}")
            print(f"Prediction: {eval_data.iloc[i]['prediction'][:150]}...")
            print(f"Similarity: {similarities[i]:.4f}")

print(f"\n{'='*60}")
print("All evaluations complete!")
print(f"ðŸ”— View full results at: http://localhost:5000")
print(f"{'='*60}")
----/code-----


touch prepare_ollama_evaluation.sh
chmod +x prepare_ollama_evaluation.sh

insert code

-----code-----

#!/bin/bash
set -euo pipefail
IFS=$'\n\t'

# Check if Ollama latest version is installed, if not install it
if ! command -v ollama &> /dev/null; then
    echo "Ollama not found. Installing Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
else
    echo "Ollama is already installed. Checking version..."
    
    # Get current installed version
    CURRENT_VERSION=$(ollama --version | grep -oP 'ollama version is \K[0-9.]+' || echo "0.0.0")
    echo "Current version: $CURRENT_VERSION"
    
    # Get latest version from GitHub API
    LATEST_VERSION=$(curl -s https://api.github.com/repos/ollama/ollama/releases/latest | grep -oP '"tag_name": "v\K[0-9.]+' || echo "999.999.999")
    echo "Latest version: $LATEST_VERSION"
    
    # Compare versions and update if needed
    if [ "$CURRENT_VERSION" != "$LATEST_VERSION" ]; then
        echo "Updating Ollama from $CURRENT_VERSION to $LATEST_VERSION..."
        curl -fsSL https://ollama.com/install.sh | sh
    else
        echo "Ollama is up to date."
    fi
fi

# pull models
ollama pull llama4
ollama pull llama3.3
ollama pull llama3.2
ollama pull mistral
ollama pull dolphin3
ollama pull deepseek-r1
ollama pull deepseek-v3.1


# Verify it's running
ollama list

----/code-----

touch evaluate_ollama.sh
chmod +x evaluate_ollama.sh

insert code:

#!/bin/bash
set -euo pipefail
IFS=$'\n\t'

# Make sure MLFLOW_TRACKING_URI is set
export MLFLOW_TRACKING_URI="http://localhost:5000"
echo "MLFLOW_TRACKING_URI is set to $MLFLOW_TRACKING_URI"

# Run the evaluation
python evaluate_ollama.py

----

run the code:

./tracking_uri_test.sh
./prepare_ollama_evaluation.sh
./evaluate_ollama.sh